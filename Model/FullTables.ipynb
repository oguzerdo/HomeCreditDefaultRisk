{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn import preprocessing\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.pandas.set_option('display.max_rows', None)\n",
    "pd.pandas.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import warnings\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.pandas.set_option('display.max_rows', None)\n",
    "pd.pandas.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "\n",
    "def application_train_test(num_rows=None, nan_as_category=False):\n",
    "    # Read data and merge\n",
    "    df = pd.read_csv(r'C:\\Users\\oe\\Desktop\\HomeCreditDefaultRisk\\data\\application_train.csv', nrows=None)\n",
    "    test_df = pd.read_csv(r'C:\\Users\\oe\\Desktop\\HomeCreditDefaultRisk\\data\\application_test.csv', nrows=None)\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
    "    df = df.append(test_df).reset_index()\n",
    "\n",
    "    # DATA PREPROCESSING\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']  # 4 gözlem değeri XNA olarak girilmiş bundan kurtarıldı.\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)  # NaN değerleri 365243 olarak girilmiş, onlar düzeltildi\n",
    "    df[\"OWN_CAR_AGE\"] = df[\"OWN_CAR_AGE\"].fillna(0)  # Araba yaş değeri boş olan gözlemler 0 olarak atandı\n",
    "\n",
    "    # FEATURE ENGINEERING\n",
    "    # AGE gün cinsinden belirtilmiş bu normal yaşa çevrildi.\n",
    "    df[\"NEW_AGE\"] = round(-1 * (df[\"DAYS_BIRTH\"] / 365), 0)\n",
    "    df[\"NEW_AGE\"] = df[\"NEW_AGE\"].astype(\"int\")\n",
    "\n",
    "    # NEW FEATURES\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']  # Müşterinin çalıştığı gün sayısının yaşına oranı\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df[\n",
    "        'AMT_CREDIT']  # Müşterinin yıllık toplam gelirinin kredi miktarına oranı\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']  # Kredinin yıllık ödemesinin, kredinin tamamına oranı\n",
    "\n",
    "    # FEATURE 1 - MAAŞ / AİLEDEKİ KİŞİ SAYISI\n",
    "    df['NEW_INC_PERS'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    # FEATURE 2 - KREDİ MİKTARI / AİLEDEKİ KİŞİ SAYISI\n",
    "    df['NEW_AMT/FAM'] = df['AMT_CREDIT'] / df['CNT_FAM_MEMBERS']\n",
    "    # FEATURE 3 - KREDİNİN YILLIK ÖDEMESİ / GELİR\n",
    "    df['NEW_ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    # FEATURE 4 - GELİR / YILLIK KREDİ * FAMILYSIZE #MODEL SONUCUNA GÖRE DEĞERLENDİR\n",
    "    df[\"NEW_FAMILY_EFFECT\"] = df['NEW_AMT/FAM'] / df['CNT_FAM_MEMBERS']\n",
    "    # FEATURE 6 - ÇEKİLEN KREDİ İLE ÜRÜN ARASINDAKİ FARKIN GELİRE ORANI ***\n",
    "    df[\"NEW_C-GP\"] = (df[\"AMT_GOODS_PRICE\"] - df[\"AMT_CREDIT\"]) / df[\"AMT_INCOME_TOTAL\"]\n",
    "    # FEATURE 7 - YAŞ / KREDİ MİKTARI\n",
    "    df[\"NEW_CREDIT/NEW_AGE\"] = df['AMT_CREDIT'] / df[\"NEW_AGE\"]\n",
    "    # FEATURE 8 - ÜRÜN / KREDİ MİKTARI ***\n",
    "    df[\"NEW_GOODS/CREDIT\"] = df[\"AMT_GOODS_PRICE\"] / df[\"AMT_CREDIT\"]\n",
    "    # FEATURE 9 - AGE / OWN_CAR_AGE\n",
    "    df[\"NEW_AGE/CAR_AGE\"] = df[\"NEW_AGE\"] / df[\"OWN_CAR_AGE\"]\n",
    "    # FEATURE 10 - EXT AĞIRLIKLI ÇARPIM\n",
    "    df['NEW_EXT_WEIGHTED'] = df.EXT_SOURCE_1 * 2 + df.EXT_SOURCE_2 * 1 + df.EXT_SOURCE_3 * 3\n",
    "    # FEATURE 11 - EXT MEAN\n",
    "    df[\"NEW_EXT_MEAN\"] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "    # FEATURE 12 - EXT STD\n",
    "    df['NEW_SCORES_STD'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
    "    df['NEW_SCORES_STD'] = df['NEW_SCORES_STD'].fillna(df['NEW_SCORES_STD'].mean())\n",
    "    # FEATURE 13 - NEW EXT PROCESS\n",
    "    # df.loc[(df[\"EXT_SOURCE_1\"] >= 0.5) | (df[\"EXT_SOURCE_2\"] >= 0.55) | (df[\"EXT_SOURCE_3\"] >= 0.45), \"NEW_BOMB\"] = 0\n",
    "    # df.loc[(df[\"EXT_SOURCE_1\"] < 0.5) | (df[\"EXT_SOURCE_2\"] < 0.55) | (df[\"EXT_SOURCE_3\"] < 0.45), \"NEW_BOMB\"] = 1\n",
    "    # FEATURE 14 - DOKUMANLARIN TOPLAMI / DOCS ATILDI\n",
    "    docs = [f for f in df.columns if 'FLAG_DOC' in f]\n",
    "    df['NEW_DOCUMENT_COUNT'] = df[docs].sum(axis=1)\n",
    "    df.drop(docs, axis=1, inplace=True)\n",
    "    # FEATURE 15 - AGE RANK 1: YOUNG 5: OLDER\n",
    "    # df[\"NEW_AGE_RANK\"] = pd.cut(x=df[\"NEW_AGE\"], bins=[0, 27, 40, 50, 65, 99], labels=[1, 2, 3, 4, 5])\n",
    "    # df[\"NEW_AGE_RANK\"] = df[\"NEW_AGE_RANK\"].astype(\"int\")\n",
    "    # df.loc[(df[\"DAYS_BIRTH\"] >= -15000),\"NEW_YOUNG_FLAG\"] = 1\n",
    "    df.drop(\"NEW_AGE\", axis=1, inplace=True)\n",
    "    # FEATURE 16 NEW_PHONE_TO_BIRTH_RATIO\n",
    "    df['NEW_PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']\n",
    "    # FEATURE 17 NEW_PHONE_TO_BIRTH_RATIO_EMPLOYER\n",
    "    df['NEW_PHONE_TO_BIRTH_RATIO_EMPLOYER'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_EMPLOYED']\n",
    "    # FEATURE 18 - NEW_INC_ORG : Sektöründeki maaş ortalamaları\n",
    "    INC_ORG = df[['AMT_INCOME_TOTAL', 'ORGANIZATION_TYPE']].groupby('ORGANIZATION_TYPE').median()['AMT_INCOME_TOTAL']\n",
    "    df['NEW_INC_ORG'] = df['ORGANIZATION_TYPE'].map(INC_ORG)\n",
    "    # FEATURE 19 - NEW_PHO/ANNU : TELEFON / YILLIK ÖDEME\n",
    "    df['NEW_PHO/ANNU'] = df['DAYS_LAST_PHONE_CHANGE'] / df['AMT_ANNUITY']\n",
    "    # FEATURE 20 - NEW_PHO/ANNU : TELEFON / KAYIT DEĞİŞTİRME\n",
    "    df[\"NEW_PHO/REG\"] = df['DAYS_LAST_PHONE_CHANGE'] * df[\"DAYS_REGISTRATION\"]\n",
    "    # FEATURE 20 - NEW_PHO/ANNU : ADRES UYUMSUZLUKLARI\n",
    "    df[\"NEW_FRAUD_1\"] = df[\"REG_CITY_NOT_LIVE_CITY\"] + df[\"REG_CITY_NOT_WORK_CITY\"] + df[\"LIVE_CITY_NOT_WORK_CITY\"]\n",
    "    df[\"NEW_FRAUD\"] = (df[\"NEW_FRAUD_1\"] + 1) * df[\"DAYS_ID_PUBLISH\"]\n",
    "    del df[\"NEW_FRAUD_1\"]\n",
    "    df[\"NEW_FRAUD_std\"] = (df[[\"REG_CITY_NOT_LIVE_CITY\", \"REG_CITY_NOT_WORK_CITY\", \"LIVE_CITY_NOT_WORK_CITY\"]]).std(axis=1)\n",
    "\n",
    "\n",
    "    # CLEAN CLASSES & LABEL ENCODING PART\n",
    "\n",
    "    df.loc[(df[\"OCCUPATION_TYPE\"] == \"Drivers\"), \"OCCUPATION_TYPE\"] = 1\n",
    "    df.loc[(df[\"OCCUPATION_TYPE\"] == \"Waiters/barmen staff\"), \"OCCUPATION_TYPE\"] = 1\n",
    "    df.loc[(df[\"OCCUPATION_TYPE\"] == \"Low-skill Laborers\"), \"OCCUPATION_TYPE\"] = 1\n",
    "    df.loc[(df[\"OCCUPATION_TYPE\"] == \"Cleaning staff\"), \"OCCUPATION_TYPE\"] = 2\n",
    "    df.loc[(df[\"OCCUPATION_TYPE\"] == \"Sales staff\"), \"OCCUPATION_TYPE\"] = 2\n",
    "    df.loc[(df[\"OCCUPATION_TYPE\"] == \"Laborers\"), \"OCCUPATION_TYPE\"] = 2\n",
    "    df.loc[(df[\"OCCUPATION_TYPE\"] == \"Security staff\"), \"OCCUPATION_TYPE\"] = 2\n",
    "    df.loc[(df[\"OCCUPATION_TYPE\"] == \"Cooking staff\"), \"OCCUPATION_TYPE\"] = 2\n",
    "    df.loc[(df[\"OCCUPATION_TYPE\"] == \"Medicine staff\"), \"OCCUPATION_TYPE\"] = 3\n",
    "    df.loc[(df[\"OCCUPATION_TYPE\"] == \"Private service staff\"), \"OCCUPATION_TYPE\"] = 3\n",
    "    df.loc[(df[\"OCCUPATION_TYPE\"] == \"Realty agents\"), \"OCCUPATION_TYPE\"] = 3\n",
    "    df.loc[(df[\"OCCUPATION_TYPE\"] == \"Secretaries\"), \"OCCUPATION_TYPE\"] = 3\n",
    "    df.loc[(df[\"OCCUPATION_TYPE\"] == \"Accountants\"), \"OCCUPATION_TYPE\"] = 4\n",
    "    df.loc[(df[\"OCCUPATION_TYPE\"] == \"Core staff\"), \"OCCUPATION_TYPE\"] = 4\n",
    "    df.loc[(df[\"OCCUPATION_TYPE\"] == \"HR staff\"), \"OCCUPATION_TYPE\"] = 4\n",
    "    df.loc[(df[\"OCCUPATION_TYPE\"] == \"High skill tech staff\"), \"OCCUPATION_TYPE\"] = 4\n",
    "    df.loc[(df[\"OCCUPATION_TYPE\"] == \"Managers\"), \"OCCUPATION_TYPE\"] = 4\n",
    "    df.loc[(df[\"OCCUPATION_TYPE\"] == \"Medicine staff\"), \"OCCUPATION_TYPE\"] = 4\n",
    "    df.loc[(df[\"OCCUPATION_TYPE\"] == \"Private service staff\"), \"OCCUPATION_TYPE\"] = 4\n",
    "    df.loc[(df[\"OCCUPATION_TYPE\"] == \"Realty agents\"), \"OCCUPATION_TYPE\"] = 4\n",
    "    df.loc[(df[\"OCCUPATION_TYPE\"] == \"Secretaries\"), \"OCCUPATION_TYPE\"] = 4\n",
    "    df.loc[(df[\"OCCUPATION_TYPE\"] == \"IT staff\"), \"OCCUPATION_TYPE\"] = 4\n",
    "\n",
    "\n",
    "    def cols(dataframe, target, noc=10, ID=True):\n",
    "        \"\"\"\n",
    "        noc : number of classes to threshold\n",
    "        ID : if your data has ID, index etc\n",
    "        \"\"\"\n",
    "        vars_more_classes = []\n",
    "        if ID:\n",
    "            ID = dataframe.columns[0]\n",
    "        else:\n",
    "            ID = \"x\"\n",
    "\n",
    "        cat_cols = [col for col in dataframe.columns if dataframe[col].nunique() < noc\n",
    "                    and col not in target]\n",
    "\n",
    "        num_cols = [col for col in dataframe.columns if dataframe[col].nunique() > noc\n",
    "                    and dataframe[col].dtypes != \"O\"\n",
    "                    and col not in target\n",
    "                    and col not in cat_cols and col not in ID]\n",
    "\n",
    "        other_cols = [col for col in dataframe.columns if col not in cat_cols\n",
    "                      and col not in num_cols and col not in ID\n",
    "                      and col not in target]\n",
    "        return cat_cols, num_cols, other_cols\n",
    "\n",
    "\n",
    "    drop_list = [\n",
    "        'FLAG_EMP_PHONE', 'FLAG_MOBIL', 'FLAG_CONT_MOBILE',\n",
    "        'LIVE_REGION_NOT_WORK_REGION', 'FLAG_EMAIL', 'FLAG_PHONE',\n",
    "        'FLAG_OWN_REALTY', 'NAME_TYPE_SUITE',\n",
    "        'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
    "        'COMMONAREA_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE',\n",
    "        'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE',\n",
    "        'NONLIVINGAREA_MODE', 'ELEVATORS_MEDI', 'EMERGENCYSTATE_MODE',\n",
    "        'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', \"REG_REGION_NOT_LIVE_REGION\",\n",
    "        \"LIVE_REGION_NOT_WORK_REGION\",\n",
    "        \"AMT_REQ_CREDIT_BUREAU_HOUR\", \"HOUR_APPR_PROCESS_START\"\n",
    "    ]\n",
    "\n",
    "    df.drop(drop_list, axis=1, inplace=True)\n",
    "\n",
    "    cat_cols, num_cols, other_cols = cols(df, \"TARGET\")\n",
    "\n",
    "\n",
    "    def rare_encoder(dataframe, rare_perc):\n",
    "        temp_df = dataframe.copy()\n",
    "\n",
    "        rare_columns = [col for col in temp_df.columns if temp_df[col].dtypes == 'O'\n",
    "                        and (temp_df[col].value_counts() / len(temp_df) < rare_perc).any(axis=None)]\n",
    "\n",
    "        for var in rare_columns:\n",
    "            tmp = temp_df[var].value_counts() / len(temp_df)\n",
    "            rare_labels = tmp[tmp < rare_perc].index\n",
    "            temp_df[var] = np.where(temp_df[var].isin(rare_labels), 'Rare', temp_df[var])\n",
    "\n",
    "        return temp_df\n",
    "\n",
    "\n",
    "    def rare_analyser(dataframe, target, rare_perc):\n",
    "        rare_columns = [col for col in dataframe.columns if\n",
    "                        dataframe[col].dtypes == \"O\" and len(dataframe[col].value_counts()) <= 20\n",
    "                        and (dataframe[col].value_counts() / len(dataframe) < rare_perc).any(axis=None)]\n",
    "\n",
    "        for var in rare_columns:\n",
    "            print(var, \":\", len(dataframe[var].value_counts()))\n",
    "            print(pd.DataFrame({\"COUNT\": dataframe[var].value_counts(),\n",
    "                                \"RATIO\": dataframe[var].value_counts() / len(dataframe),\n",
    "                                \"TARGET_MEDIAN\": dataframe.groupby(var)[target].median()}), end=\"\\n\\n\\n\")\n",
    "\n",
    "    df = rare_encoder(df, 0.10)\n",
    "\n",
    "\n",
    "    def label_encoder(dataframe, categorical_columns):\n",
    "        \"\"\"\n",
    "        2 sınıflı kategorik değişkeni 0-1 yapma\n",
    "        :param dataframe: İşlem yapılacak dataframe\n",
    "        :param categorical_columns: Label encode yapılacak kategorik değişken adları\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        labelencoder = preprocessing.LabelEncoder()\n",
    "\n",
    "        for col in categorical_columns:\n",
    "            if dataframe[col].nunique() == 2:\n",
    "                dataframe[col] = labelencoder.fit_transform(dataframe[col])\n",
    "        return dataframe\n",
    "\n",
    "\n",
    "    df = label_encoder(df, cat_cols)\n",
    "\n",
    "    df, app_cols = one_hot_encoder(df, True)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess bureau.csv and bureau_balance.csv\n",
    "def bureau_and_balance(num_rows = None, nan_as_category = True):\n",
    "    bureau = pd.read_csv(r'C:\\Users\\oe\\Desktop\\HomeCreditDefaultRisk\\data\\bureau.csv', nrows = num_rows)\n",
    "    bb = pd.read_csv(r'C:\\Users\\oe\\Desktop\\HomeCreditDefaultRisk\\data\\bureau_balance.csv', nrows = num_rows)\n",
    "     \n",
    "    # RARE ENCODING\n",
    "\n",
    "    bureau.loc[(bureau[\"CREDIT_TYPE\"] == \"Microloan\"),\"CREDIT_TYPE\"] = \"Rare\"\n",
    "    bureau.loc[(bureau[\"CREDIT_TYPE\"] == \"Loan for business development\"),\"CREDIT_TYPE\"] = \"Rare\"\n",
    "    bureau.loc[(bureau[\"CREDIT_TYPE\"] == \"Another type of loan\"),\"CREDIT_TYPE\"] = \"Rare\"\n",
    "    bureau.loc[(bureau[\"CREDIT_TYPE\"] == \"Loan for working capital replenishment\"),\"CREDIT_TYPE\"] = \"Rare\"\n",
    "    bureau.loc[(bureau[\"CREDIT_TYPE\"] == \"Unknown type of loan\"),\"CREDIT_TYPE\"] = \"Rare\"\n",
    "    bureau.loc[(bureau[\"CREDIT_TYPE\"] == \"Cash loan (non-earmarked)\"),\"CREDIT_TYPE\"] = \"Rare\"\n",
    "    bureau.loc[(bureau[\"CREDIT_TYPE\"] == \"Real estate loan\"),\"CREDIT_TYPE\"] = \"Rare\"\n",
    "    bureau.loc[(bureau[\"CREDIT_TYPE\"] == \"Loan for the purchase of equipment\"),\"CREDIT_TYPE\"] = \"Rare\"\n",
    "    #credit active or not\n",
    "    bureau.loc[(bureau[\"CREDIT_ACTIVE\"] == \"Bad debt\"),\"CREDIT_ACTIVE\"] = \"Active\"\n",
    "    bureau.loc[(bureau[\"CREDIT_ACTIVE\"] == \"Sold\"),\"CREDIT_ACTIVE\"] = \"Active\"\n",
    "\n",
    "    # GECİKMİŞ BORCU OLANLAR BORÇ MİKTARINA GÖRE ETİKETLENDİ\n",
    "    bureau[\"AMT_CREDIT_MAX_OVERDUE\"] = bureau[\"AMT_CREDIT_MAX_OVERDUE\"].fillna(0) #nan değerler borcu yok diye değerlendirildi. model sonucuna göre kontrol edilecek.\n",
    "    bureau.loc[((bureau[\"AMT_CREDIT_MAX_OVERDUE\"] >= 0) | (bureau[\"AMT_CREDIT_SUM_OVERDUE\"] >= 0)), \"NEW_DANGER\"] = 0 #gecikmiş borcu olmayanlar \n",
    "    bureau.loc[((bureau[\"AMT_CREDIT_MAX_OVERDUE\"] >= 1) | (bureau[\"AMT_CREDIT_SUM_OVERDUE\"] >= 1)), \"NEW_DANGER\"] = 1 #100.000'e kadar gecikmiş (başka) kredi borcu olanlar \n",
    "    bureau.loc[((bureau[\"AMT_CREDIT_MAX_OVERDUE\"] >= 100000) | (bureau[\"AMT_CREDIT_SUM_OVERDUE\"] >= 100000)), \"NEW_DANGER\"] = 2 #100.000'den 500000'e kadar gecikmiş (başka) kredi borcu olanlar \n",
    "    bureau.loc[((bureau[\"AMT_CREDIT_MAX_OVERDUE\"] >= 500000) | (bureau[\"AMT_CREDIT_SUM_OVERDUE\"] >= 500000)), \"NEW_DANGER\"] = 3 #500.000 den fazla gecikmiş borcu olup hayal kuranlar\n",
    "\n",
    "    # \"CNT_CREDIT_PROLONG\" kredisi kaç kez uzatıldı? çeşitlilik yok.  Uzatma veya uzatmama durumuna indirgendi\n",
    "    bureau.loc[(bureau[\"CNT_CREDIT_PROLONG\"] == 0),\"CNT_CREDIT_PROLONG\"] = 0 #diğer yerlerdeki kredisini uzatmamış kişiler\n",
    "    bureau.loc[(bureau[\"CNT_CREDIT_PROLONG\"] != 0),\"CNT_CREDIT_PROLONG\"] = 1 #diğer yerlerdeki kredisini uzatmış kişiler\n",
    "\n",
    "    # currency kategorilerdeki gözlem sayıları düşük olduğu için 1 ve 2 şeklinde kodlandı\n",
    "    bureau.loc[(bureau[\"CREDIT_CURRENCY\"] == \"currency 1\"),\"CREDIT_CURRENCY\"] = 0 #currency1\n",
    "    bureau.loc[(bureau[\"CREDIT_CURRENCY\"] == \"currency 2\"),\"CREDIT_CURRENCY\"] = 1 #currency2\n",
    "    bureau.loc[(bureau[\"CREDIT_CURRENCY\"] == \"currency 3\"),\"CREDIT_CURRENCY\"] = 1 #currency3\n",
    "    bureau.loc[(bureau[\"CREDIT_CURRENCY\"] == \"currency 4\"),\"CREDIT_CURRENCY\"] = 1 #currency4\n",
    "\n",
    "    # FEATURE 1: BUREAU_LOAN_COUNT | Bir kişinin aldığı farklı kredi sayıları\n",
    "    grp = bureau[['SK_ID_CURR', 'DAYS_CREDIT']].groupby(by = ['SK_ID_CURR'])['DAYS_CREDIT'].count().reset_index().rename(index=str, columns={'DAYS_CREDIT': 'NEW_BUREAU_LOAN_COUNT'})\n",
    "    bureau = bureau.merge(grp, on = ['SK_ID_CURR'], how = 'left')\n",
    "\n",
    "    # FEATURE 2: BUREAU_LOAN_TYPES | Bir kişinin kaç farklı tipte krediye sahip olduğu\n",
    "    grp = bureau[['SK_ID_CURR', 'CREDIT_TYPE']].groupby(by = ['SK_ID_CURR'])['CREDIT_TYPE'].nunique().reset_index().rename(index=str, columns={'CREDIT_TYPE': 'NEW_BUREAU_LOAN_TYPES'})\n",
    "    bureau = bureau.merge(grp, on = ['SK_ID_CURR'], how = 'left')\n",
    "\n",
    "    # FEATURE 3: AVERAGE_LOAN_TYPE| Bir kişinin aldığı farklı kredi türlerinin oranı\n",
    "    bureau['NEW_AVERAGE_LOAN_TYPE'] = bureau['NEW_BUREAU_LOAN_COUNT'] / bureau['NEW_BUREAU_LOAN_TYPES'] # bir kişinin aldığı farklı kredi tipi oranı. bir kişi hep aynı türden mi kredi almış, yoksa farklı türlerden mi?\n",
    "\n",
    "    # FEATURE 4: AVERAGE_LOAN_TYPE | Aktif durumda olan kredilerin % si\n",
    "    # Kredi aktif durumunu yeni bir değişkende gösteriyoruz. 0 kredileri kapanmış, 1 kredileri devam ediyor.  \n",
    "    bureau.loc[(bureau['CREDIT_ACTIVE'] == \"Closed\"), 'CREDIT_ACTIVE_BINARY'] = 0\n",
    "    bureau.loc[(bureau['CREDIT_ACTIVE'] != \"Closed\"), 'CREDIT_ACTIVE_BINARY'] = 1\n",
    "    bureau['CREDIT_ACTIVE_BINARY'] = bureau['CREDIT_ACTIVE_BINARY'].astype('int32')\n",
    "\n",
    "    # MÜŞTERİ başına AKTİF olan ortalama kredi sayısını hesaplayın\n",
    "    grp = bureau.groupby(by = ['SK_ID_CURR'])['CREDIT_ACTIVE_BINARY'].mean().reset_index().rename(index=str, columns={'CREDIT_ACTIVE_BINARY': 'NEW_ACTIVE_LOANS_PERCENTAGE'}) #1e yakın olması kötü oluyor\n",
    "    bureau = bureau.merge(grp, on = ['SK_ID_CURR'], how = 'left') #ana tablo ile birleştirme\n",
    "    del bureau['CREDIT_ACTIVE_BINARY'] #gereksiz olan sütun atıldı\n",
    "    gc.collect()\n",
    "\n",
    "    # FEATURE 5: NEW_DAYS_DIFF | Bir kişi kaç gün aralıkla yeni krediler almış\n",
    "    #Bir kişinin almış olduğu farklı kredileri alma günleri sıralandı\n",
    "    grp = bureau[['SK_ID_CURR', 'SK_ID_BUREAU', 'DAYS_CREDIT']].groupby(by = ['SK_ID_CURR'])\n",
    "    grp1 = grp.apply(lambda x: x.sort_values(['DAYS_CREDIT'], ascending = False)).reset_index(drop = True)#rename(index = str, columns = {'DAYS_CREDIT': 'DAYS_CREDIT_DIFF'})\n",
    "    print(\"Grouping and Sorting done\")\n",
    "    grp1['DAYS_CREDIT1'] = grp1['DAYS_CREDIT']*-1 # günler - olarak getirildiğinden + yapıldı\n",
    "    grp1['NEW_DAYS_DIFF'] = grp1.groupby(by = ['SK_ID_CURR'])['DAYS_CREDIT1'].diff() # aldığı farklı krediler arasında kaçar gün olduğu hesaplandı\n",
    "    grp1['NEW_DAYS_DIFF'] = grp1['NEW_DAYS_DIFF'].fillna(0).astype('uint32') #ilk değişkende nan geleceği için 0 ile doldurdum. diff fonksiyonunda 2. değerden 1. değer çıkarılıyor. bu sebeple ilk değerde nan geliyor.\n",
    "    del grp1['DAYS_CREDIT1'], grp1['DAYS_CREDIT'], grp1['SK_ID_CURR'] #gereksiz columnlar atıldı\n",
    "    print(\"Difference days calculated\")\n",
    "    # ana tablo ile birleştirme işlemleri\n",
    "    bureau = bureau.merge(grp1, on = ['SK_ID_BUREAU'], how = 'left')\n",
    "    print(\"Difference in Dates between Previous CB applications is CALCULATED \")\n",
    "\n",
    "    #FEATURE 6: NEW_CREDIT_ENDDATE_PERCENTAGE | ÖDEMESİ DEVAM EDEN KREDİ SAYISI ORTALAMASI\n",
    "    bureau.loc[bureau['DAYS_CREDIT_ENDDATE'] < 0,\"CREDIT_ENDDATE_BINARY\"] = 0  #ödemesi bitmiş (Closed) krediler\n",
    "    bureau.loc[bureau['DAYS_CREDIT_ENDDATE'] >= 0,\"CREDIT_ENDDATE_BINARY\"] = 1 #ödemesi devam eden (Active) krediler\n",
    "    grp = bureau.groupby(by = ['SK_ID_CURR'])['CREDIT_ENDDATE_BINARY'].mean().reset_index().rename(index=str, columns={'CREDIT_ENDDATE_BINARY': 'NEW_CREDIT_ENDDATE_PERCENTAGE'}) #ödemesi devam eden kredi sayısının ortalamaları\n",
    "    del bureau['CREDIT_ENDDATE_BINARY'] #gereksiz olan binary columnun düşürülmesi\n",
    "    bureau = bureau.merge(grp, on = ['SK_ID_CURR'], how = 'left') #ana tabloya ekleme\n",
    "\n",
    "    #FEATURE 7: NEW_AMT_PER_PAY | ÖDENEN BORÇ %Sİ\n",
    "    bureau[\"NEW_AMT_PER_PAY\"] = 1 - ((bureau[\"AMT_CREDIT_SUM\"]- bureau[\"AMT_CREDIT_SUM_DEBT\"]) / bureau[\"AMT_CREDIT_SUM\"])\n",
    "\n",
    "    #FEATURE 8: DAYS_ENDDATE_DIFF | Ödenmemiş krediler arasındaki gün farkları\n",
    "    #NOT: Groupby aggregation işleminde mean ve sum alınabilir\n",
    "    bureau['CREDIT_ENDDATE_BINARY'] = bureau['DAYS_CREDIT_ENDDATE']\n",
    "    # Ödemesi devam edenler(+) ve ödemesi bitmiş olanlar(0 veya -) değerler belirtiliyor\n",
    "    bureau.loc[(bureau[\"DAYS_CREDIT_ENDDATE\"] <= 0), \"CREDIT_ENDDATE_BINARY\"] = 0 #ödemesi bitmiş (Closed) krediler\n",
    "    bureau.loc[(bureau[\"DAYS_CREDIT_ENDDATE\"] > 0), \"CREDIT_ENDDATE_BINARY\"] = 1 #ödemesi devam eden (Active) krediler\n",
    "    #Ödemesi devam eden krediler üzerinde işlem yapılacak\n",
    "    B1 = bureau[bureau['CREDIT_ENDDATE_BINARY'] == 1]\n",
    "    del bureau[\"CREDIT_ENDDATE_BINARY\"]\n",
    "\n",
    "    #Ödemesi tamamlanmaya krediler arasındaki gün farklarının hesaplanması\n",
    "    # Create Dummy Column for CREDIT_ENDDATE \n",
    "    B1['DAYS_CREDIT_ENDDATE1'] = B1['DAYS_CREDIT_ENDDATE']\n",
    "    # Groupby Each Customer ID \n",
    "    grp = B1[['SK_ID_CURR', 'SK_ID_BUREAU', 'DAYS_CREDIT_ENDDATE1']].groupby(by = ['SK_ID_CURR'])\n",
    "    # Kredi ödeme sürelerinin kişi özelinde küçükten büyüğe sıralanması\n",
    "    grp1 = grp.apply(lambda x: x.sort_values(['DAYS_CREDIT_ENDDATE1'], ascending = True)).reset_index(drop = True) \n",
    "    del grp\n",
    "    gc.collect()\n",
    "    print(\"Grouping and Sorting done\")\n",
    "    # diff fonksiyonunda ilk satırlarda gelen nan valuler 0 ile dolduruldu.\n",
    "    grp1['DAYS_ENDDATE_DIFF'] = grp1.groupby(by = ['SK_ID_CURR'])['DAYS_CREDIT_ENDDATE1'].diff()\n",
    "    grp1['DAYS_ENDDATE_DIFF'] = grp1['DAYS_ENDDATE_DIFF'].fillna(0).astype('uint32')\n",
    "    del grp1['DAYS_CREDIT_ENDDATE1'], grp1['SK_ID_CURR']\n",
    "    gc.collect()\n",
    "    print(\"Difference days calculated\")\n",
    "\n",
    "    # ana tablo ile birleştirilmesi\n",
    "    bureau = bureau.merge(grp1, on = ['SK_ID_BUREAU'], how = 'left')\n",
    "    del grp1, B1\n",
    "    gc.collect()\n",
    "\n",
    "    #FEATURE 9: Toplam Geciken Borç / Toplam Borç\n",
    "    bureau['AMT_CREDIT_SUM_DEBT'] = bureau['AMT_CREDIT_SUM_DEBT'].fillna(0)  #nan değerler borç yok olarak alındı\n",
    "    bureau['AMT_CREDIT_SUM_OVERDUE'] = bureau['AMT_CREDIT_SUM_OVERDUE'].fillna(0) #nan değerler gecikme yok olarak alındır\n",
    "    #grp1 bir kişinin toplam borcu\n",
    "    #grp2 bir kişinin toplam gecikmiş borcu\n",
    "    grp1 = bureau[['SK_ID_CURR', 'AMT_CREDIT_SUM_DEBT']].groupby(by = ['SK_ID_CURR'])['AMT_CREDIT_SUM_DEBT'].sum().reset_index().rename( index = str, columns = { 'AMT_CREDIT_SUM_DEBT': 'TOTAL_CUSTOMER_DEBT'})\n",
    "    grp2 = bureau[['SK_ID_CURR', 'AMT_CREDIT_SUM_OVERDUE']].groupby(by = ['SK_ID_CURR'])['AMT_CREDIT_SUM_OVERDUE'].sum().reset_index().rename( index = str, columns = { 'AMT_CREDIT_SUM_OVERDUE': 'TOTAL_CUSTOMER_OVERDUE'})\n",
    "    #ana tabloya ekleme\n",
    "    bureau = bureau.merge(grp1, on = ['SK_ID_CURR'], how = 'left') \n",
    "    bureau = bureau.merge(grp2, on = ['SK_ID_CURR'], how = 'left')\n",
    "    del grp1, grp2\n",
    "    gc.collect()\n",
    "    bureau['NEW_OVERDUE_DEBT_RATIO'] = bureau['TOTAL_CUSTOMER_OVERDUE']/bureau['TOTAL_CUSTOMER_DEBT'] # kişinin toplam gecikmiş borcunun, toplam borcuna oranı\n",
    "    del bureau['TOTAL_CUSTOMER_OVERDUE'], bureau['TOTAL_CUSTOMER_DEBT'] #gereksiz üretilen sütunlar kaldırıldı\n",
    "    gc.collect()\n",
    "    \n",
    "    ### Bureau Balance \n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess previous_applications.csv\n",
    "def previous_applications(num_rows = None, nan_as_category = True):\n",
    "    prev = pd.read_csv(r'C:\\Users\\oe\\Desktop\\HomeCreditDefaultRisk\\data\\previous_application.csv', nrows = num_rows)\n",
    "    # Days 365.243 values -> nan\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    #FEATURE ENGINEERING\n",
    "    # ATILACAK  NFLAG_INSURED_ON_APPROVAL\n",
    "    prev = prev.drop([\"FLAG_LAST_APPL_PER_CONTRACT\"],axis=1)\n",
    "    prev = prev.drop([\"NFLAG_LAST_APPL_IN_DAY\"],axis=1)\n",
    "    prev = prev.drop([\"WEEKDAY_APPR_PROCESS_START\"],axis=1)\n",
    "    prev = prev.drop([\"NAME_TYPE_SUITE\"],axis=1)\n",
    "    prev = prev.drop([\"NFLAG_INSURED_ON_APPROVAL\"],axis=1)\n",
    "    prev = prev.drop([\"NAME_SELLER_INDUSTRY\"],axis=1)\n",
    "    \n",
    "    # AZ RASTLANAN DEĞİŞKENLER KENDİLERİNE YAKIN SINIFLARA AKTARILDILAR.\n",
    "    prev.loc[(prev[\"NAME_PAYMENT_TYPE\"] == \"Cashless from the account of the employer\"), \"NAME_PAYMENT_TYPE\"] = \"Cash through the bank \"\n",
    "    # CODE_REJECT_REASON\n",
    "    prev.loc[(prev[\"CODE_REJECT_REASON\"] != \"CLIENT\") & (prev[\"CODE_REJECT_REASON\"] != \"XAP\"), \"CODE_REJECT_REASON\"] = \"HC\"\n",
    "    # CHANNEL_TYPE\n",
    "    prev.loc[(prev[\"CHANNEL_TYPE\"] == \"Regional / Local\"), \"CHANNEL_TYPE\"] = \"Regional / Local / Stone\"\n",
    "    prev.loc[(prev[\"CHANNEL_TYPE\"] == \"Stone\"), \"CHANNEL_TYPE\"] = \"Regional / Local / Stone\"   \n",
    "    prev.loc[(prev[\"CHANNEL_TYPE\"] == \"Credit and cash offices\"), \"CHANNEL_TYPE\"] = \"Credit and cash offices / Channel\"\n",
    "    prev.loc[(prev[\"CHANNEL_TYPE\"] == \"Channel of corporate sales\"), \"CHANNEL_TYPE\"] = \"Credit and cash offices / Channel\"\n",
    "    # NAME_YIELD_GROUP\n",
    "    prev.loc[(prev[\"NAME_YIELD_GROUP\"] == \"low_action\"), \"NAME_YIELD_GROUP\" ] = \"low\"\n",
    "    prev.loc[(prev[\"NAME_YIELD_GROUP\"] == \"low_normal\"), \"NAME_YIELD_GROUP\"] = \"low\"\n",
    "    prev.loc[(prev[\"NAME_YIELD_GROUP\"] == \"high\"), \"NAME_YIELD_GROUP\"] = \"mid/high\"\n",
    "    prev.loc[(prev[\"NAME_YIELD_GROUP\"] == \"middle\"), \"NAME_YIELD_GROUP\"] = \"mid/high\"\n",
    "    # NAME_PORTFOLIO\n",
    "    prev.loc[(prev[\"NAME_PORTFOLIO\"] == \"Cards\"), \"NAME_PORTFOLIO\"] = \"Cards/Car/Cash\"\n",
    "    prev.loc[(prev[\"NAME_PORTFOLIO\"] == \"Cars\"), \"NAME_PORTFOLIO\"] = \"Cards/Car/Cash\"\n",
    "    prev.loc[(prev[\"NAME_PORTFOLIO\"] == \"Cash\"), \"NAME_PORTFOLIO\"] = \"Cards/Car/Cash\"\n",
    "\n",
    "    # Kabul edilme ve edilmeme durumuna göre ikiye ayrıldı ve verisetini değerlendirmek amacıyla target olarak kullanıldı.\n",
    "    prev.loc[(prev[\"NAME_CONTRACT_STATUS\"] == \"Approved\"),\"NAME_CONTRACT_STATUS\"] = 0\n",
    "    prev.loc[(prev[\"NAME_CONTRACT_STATUS\"] == \"Unused offer\"),\"NAME_CONTRACT_STATUS\"] = 0\n",
    "    prev.loc[(prev[\"NAME_CONTRACT_STATUS\"] == \"Canceled\"),\"NAME_CONTRACT_STATUS\"] = 1\n",
    "    prev.loc[(prev[\"NAME_CONTRACT_STATUS\"] == \"Refused\"),\"NAME_CONTRACT_STATUS\"] = 1\n",
    "    prev[\"NAME_CONTRACT_STATUS\"] = prev[\"NAME_CONTRACT_STATUS\"].astype(\"int\")\n",
    "    \n",
    "    #FEATURE 1 - NEW_APP_CREDIT_PERC | İstenen kredi ve aldığı kredi oranı\n",
    "    prev['NEW_APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    \n",
    "    #FEATURE 2 - NEW_CREDIBILITY | müşterilerin cevap alma hızları & ONAY durumları\n",
    "    prev[\"DAYS_DECISION2\"] = prev[\"DAYS_DECISION\"] * -1\n",
    "    prev['ANS_SPEED'] = pd.cut(x = prev['DAYS_DECISION2'], bins = [0,100, 700, 3000], labels = [\"Fast\", \"Normal\", \"Late\"])\n",
    "    del prev[\"DAYS_DECISION2\"]\n",
    "\n",
    "    prev.loc[((prev[\"NAME_CONTRACT_STATUS\"] == \"Approved\" ) & (prev['ANS_SPEED'] == \"Fast\")),\"NEW_CREDIBILITY\"] = 5 #hızlı ve olumlu onay alanlar \n",
    "    prev.loc[((prev[\"NAME_CONTRACT_STATUS\"] == \"Approved\" ) & (prev['ANS_SPEED'] == \"Normal\")),\"NEW_CREDIBILITY\"] = 4 #normal ve olumlu onay alanlar \n",
    "    prev.loc[((prev[\"NAME_CONTRACT_STATUS\"] == \"Approved\" ) & (prev['ANS_SPEED'] == \"Late\")),\"NEW_CREDIBILITY\"] = 3 #yavaş ve olumlu onay alanlar \n",
    "    prev.loc[((prev[\"NAME_CONTRACT_STATUS\"] == \"Refused\" ) & (prev['ANS_SPEED'] == \"Late\")),\"NEW_CREDIBILITY\"] = 2  #yavaş ve olumsuz onay alanlar \n",
    "    prev.loc[((prev[\"NAME_CONTRACT_STATUS\"] == \"Refused\" ) & (prev['ANS_SPEED'] == \"Normal\")),\"NEW_CREDIBILITY\"] = 1  #normal ve olumsuz onay alanlar \n",
    "    prev.loc[((prev[\"NAME_CONTRACT_STATUS\"] == \"Refused\" ) & (prev['ANS_SPEED'] == \"Fast\")),\"NEW_CREDIBILITY\"] = 0  #hızlı ve olumsuz onay alanlar \n",
    "    \n",
    "    #FEATURE 3 - \"NEW_ANN/CDT\" | müşteri maaşının kredi tutarına oranı\n",
    "    prev[\"NEW_ANN/CDT_PERC\"] = prev[\"AMT_ANNUITY\"] / prev[\"AMT_CREDIT\"]\n",
    "    \n",
    "    #FEATURE 4 - \"NEW_ANN/CDT\" | tam tutarın vadeye bölümü\n",
    "    prev[\"NEW_CDT/PAY\"] = prev[\"AMT_CREDIT\"] / prev[\"CNT_PAYMENT\"]\n",
    "    \n",
    "    # FEATURE 5 - \"NEW_ABILITY\" | müşteri geçmiş aylık kredi tutarının maaşına oranı\n",
    "    prev[\"NEW_PAY_ABILITY_PERC\"] = prev[\"NEW_CDT/PAY\"] / prev[\"AMT_ANNUITY\"] \n",
    "    \n",
    "    # FEATURE 6 - \"NEW_ABILITY\" | peşinat / maaş\n",
    "    prev[\"NEW_PAY_ANN/DOWN_PERC\"] = prev[\"AMT_ANNUITY\"] / prev[\"AMT_DOWN_PAYMENT\"] \n",
    "    \n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "        'NEW_APP_CREDIT_PERC': ['min', 'max', 'mean'],\n",
    "        'NEW_CREDIBILITY': [\"sum\",\"mean\"],\n",
    "        'NEW_ANN/CDT_PERC': ['min', 'max', 'mean'],\n",
    "        'NEW_CDT/PAY': ['min', 'max', 'mean'],\n",
    "        'NEW_PAY_ABILITY_PERC': ['min', 'max', 'mean'],\n",
    "        'NEW_PAY_ANN/DOWN_PERC': ['min', 'max', 'mean']\n",
    "    }\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev[\"NAME_CONTRACT_STATUS\"] == 0]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev[\"NAME_CONTRACT_STATUS\"] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "    return prev_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_cash(num_rows = None, nan_as_category = True):\n",
    "    pos = pd.read_csv(r'C:\\Users\\oe\\Desktop\\HomeCreditDefaultRisk\\data\\POS_CASH_balance.csv', nrows = num_rows)\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    # Count pos cash accounts\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "    del pos\n",
    "    gc.collect()\n",
    "    return pos_agg\n",
    "    \n",
    "# Preprocess installments_payments.csv\n",
    "def installments_payments(num_rows = None, nan_as_category = True):\n",
    "    ins = pd.read_csv(r'C:\\Users\\oe\\Desktop\\HomeCreditDefaultRisk\\data\\installments_payments.csv', nrows = num_rows)\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    return ins_agg\n",
    "\n",
    "# Preprocess credit_card_balance.csv\n",
    "def credit_card_balance(num_rows = None, nan_as_category = True):\n",
    "    cc = pd.read_csv(r'C:\\Users\\oe\\Desktop\\HomeCreditDefaultRisk\\data\\credit_card_balance.csv', nrows = num_rows)\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n",
    "    # General aggregations\n",
    "    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    # Count credit card lines\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM GBDT with KFold or Stratified KFold\n",
    "# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\n",
    "def kfold_lightgbm(df, num_folds, stratified = False, debug= False):\n",
    "    # Divide in training/validation and test data\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    test_df = df[df['TARGET'].isnull()]\n",
    "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "    del df\n",
    "    gc.collect()\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "\n",
    "        # LightGBM parameters found by Bayesian optimization\n",
    "        clf = LGBMClassifier(\n",
    "            nthread=6,\n",
    "            n_estimators=10000,\n",
    "            learning_rate=0.02,\n",
    "            num_leaves=34,\n",
    "            colsample_bytree=0.9497036,\n",
    "            subsample=0.8715623,\n",
    "            max_depth=8,\n",
    "            reg_alpha=0.041545473,\n",
    "            reg_lambda=0.0735294,\n",
    "            min_split_gain=0.0222415,\n",
    "            min_child_weight=39.3259775,\n",
    "            silent=-1,\n",
    "            verbose=-1, )\n",
    "\n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "            eval_metric= 'auc', verbose= 200, early_stopping_rounds= 200)\n",
    "\n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n",
    "    # Write submission file and plot feature importance\n",
    "    if not debug:\n",
    "        test_df['TARGET'] = sub_preds\n",
    "        test_df[['SK_ID_CURR', 'TARGET']].to_csv(submission_file_name, index= False)\n",
    "    display_importances(feature_importance_df)\n",
    "    return feature_importance_df\n",
    "\n",
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances01.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 307511, test samples: 48744\n",
      "Grouping and Sorting done\n",
      "Difference days calculated\n",
      "Difference in Dates between Previous CB applications is CALCULATED \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-5b9cbb74052e>:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  B1['DAYS_CREDIT_ENDDATE1'] = B1['DAYS_CREDIT_ENDDATE']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping and Sorting done\n",
      "Difference days calculated\n",
      "Bureau df shape: (305811, 105)\n",
      "Process bureau and bureau_balance - done in 608s\n",
      "Previous applications df shape: (338857, 240)\n",
      "Process previous_applications - done in 20s\n",
      "Pos-cash balance df shape: (337252, 18)\n",
      "Process POS-CASH balance - done in 9s\n",
      "Installments payments df shape: (339587, 26)\n",
      "Process installments payments - done in 27s\n",
      "Credit card balance df shape: (103558, 141)\n",
      "Process credit card balance - done in 14s\n",
      "Starting LightGBM. Train shape: (307507, 655), test shape: (48744, 655)\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's auc: 0.796388\ttraining's binary_logloss: 0.234189\tvalid_1's auc: 0.778546\tvalid_1's binary_logloss: 0.245972\n",
      "[400]\ttraining's auc: 0.819046\ttraining's binary_logloss: 0.224912\tvalid_1's auc: 0.788106\tvalid_1's binary_logloss: 0.242402\n",
      "[600]\ttraining's auc: 0.834288\ttraining's binary_logloss: 0.21881\tvalid_1's auc: 0.791237\tvalid_1's binary_logloss: 0.241359\n",
      "[800]\ttraining's auc: 0.846346\ttraining's binary_logloss: 0.213884\tvalid_1's auc: 0.793097\tvalid_1's binary_logloss: 0.240691\n",
      "[1000]\ttraining's auc: 0.857088\ttraining's binary_logloss: 0.20937\tvalid_1's auc: 0.793762\tvalid_1's binary_logloss: 0.240429\n",
      "[1200]\ttraining's auc: 0.86677\ttraining's binary_logloss: 0.20518\tvalid_1's auc: 0.794367\tvalid_1's binary_logloss: 0.240189\n",
      "[1400]\ttraining's auc: 0.875578\ttraining's binary_logloss: 0.201211\tvalid_1's auc: 0.794646\tvalid_1's binary_logloss: 0.240163\n",
      "[1600]\ttraining's auc: 0.883724\ttraining's binary_logloss: 0.197425\tvalid_1's auc: 0.794776\tvalid_1's binary_logloss: 0.240167\n",
      "Early stopping, best iteration is:\n",
      "[1537]\ttraining's auc: 0.881218\ttraining's binary_logloss: 0.198595\tvalid_1's auc: 0.794916\tvalid_1's binary_logloss: 0.240105\n",
      "Fold  1 AUC : 0.794916\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's auc: 0.797012\ttraining's binary_logloss: 0.234706\tvalid_1's auc: 0.778226\tvalid_1's binary_logloss: 0.240707\n",
      "[400]\ttraining's auc: 0.819606\ttraining's binary_logloss: 0.225463\tvalid_1's auc: 0.787178\tvalid_1's binary_logloss: 0.237017\n",
      "[600]\ttraining's auc: 0.83529\ttraining's binary_logloss: 0.219205\tvalid_1's auc: 0.789701\tvalid_1's binary_logloss: 0.235947\n",
      "[800]\ttraining's auc: 0.847368\ttraining's binary_logloss: 0.214177\tvalid_1's auc: 0.791178\tvalid_1's binary_logloss: 0.235337\n",
      "[1000]\ttraining's auc: 0.857732\ttraining's binary_logloss: 0.209801\tvalid_1's auc: 0.791826\tvalid_1's binary_logloss: 0.235111\n",
      "[1200]\ttraining's auc: 0.867125\ttraining's binary_logloss: 0.205716\tvalid_1's auc: 0.79211\tvalid_1's binary_logloss: 0.235077\n",
      "[1400]\ttraining's auc: 0.876075\ttraining's binary_logloss: 0.201648\tvalid_1's auc: 0.792455\tvalid_1's binary_logloss: 0.234943\n",
      "Early stopping, best iteration is:\n",
      "[1316]\ttraining's auc: 0.872469\ttraining's binary_logloss: 0.20331\tvalid_1's auc: 0.792547\tvalid_1's binary_logloss: 0.234926\n",
      "Fold  2 AUC : 0.792547\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's auc: 0.797357\ttraining's binary_logloss: 0.234231\tvalid_1's auc: 0.766409\tvalid_1's binary_logloss: 0.246305\n",
      "[400]\ttraining's auc: 0.820269\ttraining's binary_logloss: 0.224897\tvalid_1's auc: 0.777084\tvalid_1's binary_logloss: 0.242626\n",
      "[600]\ttraining's auc: 0.83565\ttraining's binary_logloss: 0.21864\tvalid_1's auc: 0.780649\tvalid_1's binary_logloss: 0.241557\n",
      "[800]\ttraining's auc: 0.847634\ttraining's binary_logloss: 0.213697\tvalid_1's auc: 0.782054\tvalid_1's binary_logloss: 0.241178\n",
      "[1000]\ttraining's auc: 0.858366\ttraining's binary_logloss: 0.209104\tvalid_1's auc: 0.782943\tvalid_1's binary_logloss: 0.240944\n",
      "[1200]\ttraining's auc: 0.867843\ttraining's binary_logloss: 0.204895\tvalid_1's auc: 0.783312\tvalid_1's binary_logloss: 0.240873\n",
      "[1400]\ttraining's auc: 0.876689\ttraining's binary_logloss: 0.200923\tvalid_1's auc: 0.783182\tvalid_1's binary_logloss: 0.240988\n",
      "Early stopping, best iteration is:\n",
      "[1205]\ttraining's auc: 0.868054\ttraining's binary_logloss: 0.204806\tvalid_1's auc: 0.78334\tvalid_1's binary_logloss: 0.240867\n",
      "Fold  3 AUC : 0.783340\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's auc: 0.796674\ttraining's binary_logloss: 0.234727\tvalid_1's auc: 0.778\tvalid_1's binary_logloss: 0.240773\n",
      "[400]\ttraining's auc: 0.819385\ttraining's binary_logloss: 0.225454\tvalid_1's auc: 0.78824\tvalid_1's binary_logloss: 0.236729\n",
      "[600]\ttraining's auc: 0.83485\ttraining's binary_logloss: 0.219296\tvalid_1's auc: 0.791612\tvalid_1's binary_logloss: 0.235427\n",
      "[800]\ttraining's auc: 0.847261\ttraining's binary_logloss: 0.214221\tvalid_1's auc: 0.793042\tvalid_1's binary_logloss: 0.234889\n",
      "[1000]\ttraining's auc: 0.857885\ttraining's binary_logloss: 0.209653\tvalid_1's auc: 0.793765\tvalid_1's binary_logloss: 0.234616\n",
      "[1200]\ttraining's auc: 0.86763\ttraining's binary_logloss: 0.205339\tvalid_1's auc: 0.794217\tvalid_1's binary_logloss: 0.234432\n",
      "[1400]\ttraining's auc: 0.876526\ttraining's binary_logloss: 0.20133\tvalid_1's auc: 0.79453\tvalid_1's binary_logloss: 0.234318\n",
      "[1600]\ttraining's auc: 0.884562\ttraining's binary_logloss: 0.197565\tvalid_1's auc: 0.794782\tvalid_1's binary_logloss: 0.234169\n",
      "[1800]\ttraining's auc: 0.892106\ttraining's binary_logloss: 0.193889\tvalid_1's auc: 0.794822\tvalid_1's binary_logloss: 0.234101\n",
      "[2000]\ttraining's auc: 0.898805\ttraining's binary_logloss: 0.190492\tvalid_1's auc: 0.794755\tvalid_1's binary_logloss: 0.234088\n",
      "Early stopping, best iteration is:\n",
      "[1825]\ttraining's auc: 0.89304\ttraining's binary_logloss: 0.193441\tvalid_1's auc: 0.794893\tvalid_1's binary_logloss: 0.234075\n",
      "Fold  4 AUC : 0.794893\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's auc: 0.796309\ttraining's binary_logloss: 0.235599\tvalid_1's auc: 0.779173\tvalid_1's binary_logloss: 0.233566\n",
      "[400]\ttraining's auc: 0.818979\ttraining's binary_logloss: 0.226297\tvalid_1's auc: 0.788942\tvalid_1's binary_logloss: 0.229734\n",
      "[600]\ttraining's auc: 0.834414\ttraining's binary_logloss: 0.22011\tvalid_1's auc: 0.792785\tvalid_1's binary_logloss: 0.228397\n",
      "[800]\ttraining's auc: 0.846783\ttraining's binary_logloss: 0.215071\tvalid_1's auc: 0.794698\tvalid_1's binary_logloss: 0.227753\n",
      "[1000]\ttraining's auc: 0.857483\ttraining's binary_logloss: 0.210531\tvalid_1's auc: 0.795696\tvalid_1's binary_logloss: 0.227432\n",
      "[1200]\ttraining's auc: 0.867176\ttraining's binary_logloss: 0.206296\tvalid_1's auc: 0.796264\tvalid_1's binary_logloss: 0.22717\n",
      "Early stopping, best iteration is:\n",
      "[1181]\ttraining's auc: 0.866306\ttraining's binary_logloss: 0.206688\tvalid_1's auc: 0.796308\tvalid_1's binary_logloss: 0.227173\n",
      "Fold  5 AUC : 0.796308\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's auc: 0.797282\ttraining's binary_logloss: 0.234539\tvalid_1's auc: 0.768143\tvalid_1's binary_logloss: 0.243106\n",
      "[400]\ttraining's auc: 0.819966\ttraining's binary_logloss: 0.225228\tvalid_1's auc: 0.777668\tvalid_1's binary_logloss: 0.239823\n",
      "[600]\ttraining's auc: 0.835449\ttraining's binary_logloss: 0.21897\tvalid_1's auc: 0.781392\tvalid_1's binary_logloss: 0.238586\n",
      "[800]\ttraining's auc: 0.847474\ttraining's binary_logloss: 0.214037\tvalid_1's auc: 0.783197\tvalid_1's binary_logloss: 0.237987\n",
      "[1000]\ttraining's auc: 0.858055\ttraining's binary_logloss: 0.209541\tvalid_1's auc: 0.783953\tvalid_1's binary_logloss: 0.237693\n",
      "[1200]\ttraining's auc: 0.867419\ttraining's binary_logloss: 0.205373\tvalid_1's auc: 0.784124\tvalid_1's binary_logloss: 0.237592\n",
      "[1400]\ttraining's auc: 0.876078\ttraining's binary_logloss: 0.201394\tvalid_1's auc: 0.784127\tvalid_1's binary_logloss: 0.237626\n",
      "Early stopping, best iteration is:\n",
      "[1238]\ttraining's auc: 0.869169\ttraining's binary_logloss: 0.204598\tvalid_1's auc: 0.784335\tvalid_1's binary_logloss: 0.237535\n",
      "Fold  6 AUC : 0.784335\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's auc: 0.796814\ttraining's binary_logloss: 0.234849\tvalid_1's auc: 0.773138\tvalid_1's binary_logloss: 0.240461\n",
      "[400]\ttraining's auc: 0.819555\ttraining's binary_logloss: 0.225565\tvalid_1's auc: 0.784094\tvalid_1's binary_logloss: 0.236339\n",
      "[600]\ttraining's auc: 0.835087\ttraining's binary_logloss: 0.21937\tvalid_1's auc: 0.787601\tvalid_1's binary_logloss: 0.235022\n",
      "[800]\ttraining's auc: 0.847361\ttraining's binary_logloss: 0.2143\tvalid_1's auc: 0.78943\tvalid_1's binary_logloss: 0.234329\n",
      "[1000]\ttraining's auc: 0.857722\ttraining's binary_logloss: 0.209837\tvalid_1's auc: 0.789805\tvalid_1's binary_logloss: 0.234177\n",
      "[1200]\ttraining's auc: 0.867187\ttraining's binary_logloss: 0.205656\tvalid_1's auc: 0.790051\tvalid_1's binary_logloss: 0.23412\n",
      "[1400]\ttraining's auc: 0.875814\ttraining's binary_logloss: 0.201679\tvalid_1's auc: 0.790131\tvalid_1's binary_logloss: 0.234114\n",
      "Early stopping, best iteration is:\n",
      "[1270]\ttraining's auc: 0.870426\ttraining's binary_logloss: 0.204168\tvalid_1's auc: 0.790324\tvalid_1's binary_logloss: 0.234027\n",
      "Fold  7 AUC : 0.790324\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's auc: 0.796964\ttraining's binary_logloss: 0.234388\tvalid_1's auc: 0.772978\tvalid_1's binary_logloss: 0.244316\n",
      "[400]\ttraining's auc: 0.81946\ttraining's binary_logloss: 0.225194\tvalid_1's auc: 0.783402\tvalid_1's binary_logloss: 0.24049\n",
      "[600]\ttraining's auc: 0.835058\ttraining's binary_logloss: 0.218874\tvalid_1's auc: 0.787272\tvalid_1's binary_logloss: 0.2391\n",
      "[800]\ttraining's auc: 0.847195\ttraining's binary_logloss: 0.213932\tvalid_1's auc: 0.788921\tvalid_1's binary_logloss: 0.238553\n",
      "[1000]\ttraining's auc: 0.857878\ttraining's binary_logloss: 0.209382\tvalid_1's auc: 0.789679\tvalid_1's binary_logloss: 0.238303\n",
      "[1200]\ttraining's auc: 0.867457\ttraining's binary_logloss: 0.205146\tvalid_1's auc: 0.790076\tvalid_1's binary_logloss: 0.238208\n",
      "[1400]\ttraining's auc: 0.87622\ttraining's binary_logloss: 0.201153\tvalid_1's auc: 0.79075\tvalid_1's binary_logloss: 0.238052\n",
      "[1600]\ttraining's auc: 0.884448\ttraining's binary_logloss: 0.197271\tvalid_1's auc: 0.791307\tvalid_1's binary_logloss: 0.237892\n",
      "[1800]\ttraining's auc: 0.891857\ttraining's binary_logloss: 0.193638\tvalid_1's auc: 0.791457\tvalid_1's binary_logloss: 0.237903\n",
      "Early stopping, best iteration is:\n",
      "[1634]\ttraining's auc: 0.885678\ttraining's binary_logloss: 0.196662\tvalid_1's auc: 0.79135\tvalid_1's binary_logloss: 0.237866\n",
      "Fold  8 AUC : 0.791350\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    }
   ],
   "source": [
    "def main(debug = False):\n",
    "    num_rows = 10000 if debug else None\n",
    "    df = application_train_test(num_rows)\n",
    "    with timer(\"Process bureau and bureau_balance\"):\n",
    "        bureau = bureau_and_balance(num_rows)\n",
    "        print(\"Bureau df shape:\", bureau.shape)\n",
    "        df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "        del bureau\n",
    "        gc.collect()\n",
    "    with timer(\"Process previous_applications\"):\n",
    "        prev = previous_applications(num_rows)\n",
    "        print(\"Previous applications df shape:\", prev.shape)\n",
    "        df = df.join(prev, how='left', on='SK_ID_CURR')\n",
    "        del prev\n",
    "        gc.collect()\n",
    "    with timer(\"Process POS-CASH balance\"):\n",
    "        pos = pos_cash(num_rows)\n",
    "        print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "        df = df.join(pos, how='left', on='SK_ID_CURR')\n",
    "        del pos\n",
    "        gc.collect()\n",
    "    with timer(\"Process installments payments\"):\n",
    "        ins = installments_payments(num_rows)\n",
    "        print(\"Installments payments df shape:\", ins.shape)\n",
    "        df = df.join(ins, how='left', on='SK_ID_CURR')\n",
    "        del ins\n",
    "        gc.collect()\n",
    "    with timer(\"Process credit card balance\"):\n",
    "        cc = credit_card_balance(num_rows)\n",
    "        print(\"Credit card balance df shape:\", cc.shape)\n",
    "        df = df.join(cc, how='left', on='SK_ID_CURR')\n",
    "        del cc\n",
    "        gc.collect()\n",
    "    with timer(\"Run LightGBM with kfold\"):\n",
    "        feat_importance = kfold_lightgbm(df, num_folds= 10, stratified= False, debug= debug)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    submission_file_name = \"submission_kernel02.csv\"\n",
    "    with timer(\"Full model run\"):\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
